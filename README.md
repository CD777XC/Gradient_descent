# Gradient_descent üóª

Gradient descent algorithm implement from scratch, using the vectorization method. 
I plotted the cost function convergence curve and evaluated my model accuracy for linear regression using the R-squared method.

You can run this notebook and print :
  - f_wb(X)
  - J
  - J_history
  - w, b

As well you can uncomment this code below to see the evolution of the cost function during gradient descent :

```
if (i+1)%1000 == 0:
    print(f"Iteration {i+1}: Cost function J = {J}")  # This will print the value of J at each iteration
```

Make sure to have your skis on because the descent is steep ‚õ∑Ô∏è
